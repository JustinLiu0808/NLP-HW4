{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Q4: data statistics helpers (no LaTeX output) =====\n",
    "import os, re\n",
    "from typing import List, Dict\n",
    "from transformers import T5TokenizerFast\n",
    "\n",
    "DATA_DIR = \"data\"  # 如路径不同改这里\n",
    "\n",
    "def _read_lines(path: str) -> List[str]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [ln.strip() for ln in f]\n",
    "    return [ln for ln in lines if ln != \"\"]\n",
    "\n",
    "def _char_stats(lines: List[str]) -> Dict[str, float]:\n",
    "    lens = [len(s) for s in lines]\n",
    "    if not lens:\n",
    "        return {\"mean\": 0.0, \"min\": 0, \"max\": 0}\n",
    "    return {\"mean\": sum(lens)/len(lens), \"min\": min(lens), \"max\": max(lens)}\n",
    "\n",
    "# 朴素分词用于“预处理前”的词表规模\n",
    "_NL_TOKEN_RE  = re.compile(r\"[A-Za-z0-9_]+\")\n",
    "_SQL_TOKEN_RE = re.compile(r\"[A-Za-z0-9_]+|[(),.*=<>!;'%+-/]+\")\n",
    "\n",
    "def _vocab_size_simple(lines: List[str], kind: str) -> int:\n",
    "    toks = []\n",
    "    if kind == \"nl\":\n",
    "        for s in lines: toks += _NL_TOKEN_RE.findall(s.lower())\n",
    "    elif kind == \"sql\":\n",
    "        for s in lines: toks += _SQL_TOKEN_RE.findall(s.lower())\n",
    "    else:\n",
    "        raise ValueError(\"kind must be 'nl' or 'sql'\")\n",
    "    return len(set(toks))\n",
    "\n",
    "def _token_len_stats_t5(lines: List[str], tokenizer: T5TokenizerFast) -> Dict[str, float]:\n",
    "    # 长度统计：包含特殊符号（更接近模型真实输入）\n",
    "    enc = tokenizer(lines, add_special_tokens=True, padding=False, truncation=False, return_length=True)\n",
    "    lens = enc[\"length\"]\n",
    "    if not lens:\n",
    "        return {\"mean\": 0.0, \"min\": 0, \"max\": 0}\n",
    "    return {\"mean\": sum(lens)/len(lens), \"min\": min(lens), \"max\": max(lens)}\n",
    "\n",
    "def _token_vocab_size_t5(lines: List[str], tokenizer: T5TokenizerFast) -> int:\n",
    "    # 词表规模：不计特殊符号\n",
    "    ids = []\n",
    "    for s in lines:\n",
    "        ids += tokenizer.encode(s, add_special_tokens=False)\n",
    "    return len(set(ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEFORE preprocessing (chars / simple vocab) ===\n",
      "Number of examples: Train=4225  Dev=466\n",
      "Mean sentence length (chars): Train=62.2  Dev=61.8\n",
      "Mean SQL query length (chars): Train=600.7  Dev=582.7\n",
      "Min/Max sentence length (chars): Train=6 / 215  Dev=10 / 139\n",
      "Min/Max SQL query length (chars): Train=73 / 1438  Dev=78 / 1370\n",
      "Vocabulary size (natural language): Train=858  Dev=443\n",
      "Vocabulary size (SQL): Train=540  Dev=340\n",
      "\n",
      "=== AFTER preprocessing (T5 tokens) ===\n",
      "Mean sentence length (tokens): Train=18.1  Dev=18.1\n",
      "Mean SQL query length (tokens): Train=217.4  Dev=211.1\n",
      "Min/Max sentence length (tokens): Train=3 / 60  Dev=4 / 44\n",
      "Min/Max SQL query length (tokens): Train=26 / 511  Dev=31 / 503\n",
      "Vocabulary size (natural language, tokens): Train=791  Dev=465\n",
      "Vocabulary size (SQL, tokens): Train=555  Dev=395\n"
     ]
    }
   ],
   "source": [
    "# ===== Compute and print numbers only =====\n",
    "# 1) 读取\n",
    "train_nl  = _read_lines(os.path.join(DATA_DIR, \"train.nl\"))\n",
    "dev_nl    = _read_lines(os.path.join(DATA_DIR, \"dev.nl\"))\n",
    "train_sql = _read_lines(os.path.join(DATA_DIR, \"train.sql\"))\n",
    "dev_sql   = _read_lines(os.path.join(DATA_DIR, \"dev.sql\"))\n",
    "\n",
    "# 2) 预处理前（字符 & 简单词表）\n",
    "before_train = {\n",
    "    \"num_examples\": len(train_nl),\n",
    "    \"sent_chars\": _char_stats(train_nl),\n",
    "    \"sql_chars\":  _char_stats(train_sql),\n",
    "    \"vocab_nl\":   _vocab_size_simple(train_nl, \"nl\"),\n",
    "    \"vocab_sql\":  _vocab_size_simple(train_sql, \"sql\"),\n",
    "}\n",
    "before_dev = {\n",
    "    \"num_examples\": len(dev_nl),\n",
    "    \"sent_chars\": _char_stats(dev_nl),\n",
    "    \"sql_chars\":  _char_stats(dev_sql),\n",
    "    \"vocab_nl\":   _vocab_size_simple(dev_nl, \"nl\"),\n",
    "    \"vocab_sql\":  _vocab_size_simple(dev_sql, \"sql\"),\n",
    "}\n",
    "\n",
    "# 3) 预处理后（T5 tokenizer 的 token 级）\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "after_train = {\n",
    "    \"sent_tokens\": _token_len_stats_t5(train_nl, tokenizer),\n",
    "    \"sql_tokens\":  _token_len_stats_t5(train_sql, tokenizer),\n",
    "    \"vocab_nl_tokens\":  _token_vocab_size_t5(train_nl, tokenizer),\n",
    "    \"vocab_sql_tokens\": _token_vocab_size_t5(train_sql, tokenizer),\n",
    "}\n",
    "after_dev = {\n",
    "    \"sent_tokens\": _token_len_stats_t5(dev_nl, tokenizer),\n",
    "    \"sql_tokens\":  _token_len_stats_t5(dev_sql, tokenizer),\n",
    "    \"vocab_nl_tokens\":  _token_vocab_size_t5(dev_nl, tokenizer),\n",
    "    \"vocab_sql_tokens\": _token_vocab_size_t5(dev_sql, tokenizer),\n",
    "}\n",
    "\n",
    "# 4) 打印（保留 1 位小数，方便你直接粘）\n",
    "def _fmt(x): \n",
    "    return f\"{x:.1f}\" if isinstance(x, float) else str(x)\n",
    "\n",
    "print(\"=== BEFORE preprocessing (chars / simple vocab) ===\")\n",
    "print(f\"Number of examples: Train={before_train['num_examples']}  Dev={before_dev['num_examples']}\")\n",
    "print(f\"Mean sentence length (chars): Train={_fmt(before_train['sent_chars']['mean'])}  Dev={_fmt(before_dev['sent_chars']['mean'])}\")\n",
    "print(f\"Mean SQL query length (chars): Train={_fmt(before_train['sql_chars']['mean'])}  Dev={_fmt(before_dev['sql_chars']['mean'])}\")\n",
    "print(f\"Min/Max sentence length (chars): Train={before_train['sent_chars']['min']} / {before_train['sent_chars']['max']}  \"\n",
    "      f\"Dev={before_dev['sent_chars']['min']} / {before_dev['sent_chars']['max']}\")\n",
    "print(f\"Min/Max SQL query length (chars): Train={before_train['sql_chars']['min']} / {before_train['sql_chars']['max']}  \"\n",
    "      f\"Dev={before_dev['sql_chars']['min']} / {before_dev['sql_chars']['max']}\")\n",
    "print(f\"Vocabulary size (natural language): Train={before_train['vocab_nl']}  Dev={before_dev['vocab_nl']}\")\n",
    "print(f\"Vocabulary size (SQL): Train={before_train['vocab_sql']}  Dev={before_dev['vocab_sql']}\")\n",
    "\n",
    "print(\"\\n=== AFTER preprocessing (T5 tokens) ===\")\n",
    "print(f\"Mean sentence length (tokens): Train={_fmt(after_train['sent_tokens']['mean'])}  Dev={_fmt(after_dev['sent_tokens']['mean'])}\")\n",
    "print(f\"Mean SQL query length (tokens): Train={_fmt(after_train['sql_tokens']['mean'])}  Dev={_fmt(after_dev['sql_tokens']['mean'])}\")\n",
    "print(f\"Min/Max sentence length (tokens): Train={after_train['sent_tokens']['min']} / {after_train['sent_tokens']['max']}  \"\n",
    "      f\"Dev={after_dev['sent_tokens']['min']} / {after_dev['sent_tokens']['max']}\")\n",
    "print(f\"Min/Max SQL query length (tokens): Train={after_train['sql_tokens']['min']} / {after_train['sql_tokens']['max']}  \"\n",
    "      f\"Dev={after_dev['sql_tokens']['min']} / {after_dev['sql_tokens']['max']}\")\n",
    "print(f\"Vocabulary size (natural language, tokens): Train={after_train['vocab_nl_tokens']}  Dev={after_dev['vocab_nl_tokens']}\")\n",
    "print(f\"Vocabulary size (SQL, tokens): Train={after_train['vocab_sql_tokens']}  Dev={after_dev['vocab_sql_tokens']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
